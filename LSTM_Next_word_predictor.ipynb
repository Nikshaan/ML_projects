{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4hEeBzYGAHF"
      },
      "source": [
        "## Importing data from kaggle\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "83rt1HENEjAz"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIhmJgLgFmfi",
        "outputId": "0becaade-ed5c-4b6c-bc89-95199d47d04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/ashishpandey2062/next-word-predictor-text-generator-dataset\n",
            "License(s): MIT\n",
            "Downloading next-word-predictor-text-generator-dataset.zip to /content\n",
            "  0% 0.00/61.5k [00:00<?, ?B/s]\n",
            "100% 61.5k/61.5k [00:00<00:00, 223MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d  ashishpandey2062/next-word-predictor-text-generator-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b3so2GD9GTHU"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/next-word-predictor-text-generator-dataset.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JyBkcjm7K3jS"
      },
      "outputs": [],
      "source": [
        "with open('/content/next_word_predictor.txt', 'r') as file:\n",
        "    text_data = file.read()\n",
        "    text_words = text_data.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OqmqQaQlGyJV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqZsJD4DMCRh"
      },
      "source": [
        "## Converting words into tokens (integer values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2f5GbBwaIyxx"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lDO8wZm9I1mp"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(text_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rSoM_3lSJGhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6f8022-ec84-4534-d7a7-f416e1aa42a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'i': 6,\n",
              " 'you': 7,\n",
              " 'in': 8,\n",
              " 'is': 9,\n",
              " 'monica': 10,\n",
              " 'it': 11,\n",
              " 'with': 12,\n",
              " 'ross': 13,\n",
              " 'that': 14,\n",
              " 'rachel': 15,\n",
              " 'for': 16,\n",
              " 'chandler': 17,\n",
              " 'this': 18,\n",
              " 'on': 19,\n",
              " 'joey': 20,\n",
              " 'was': 21,\n",
              " 'oh': 22,\n",
              " 'phoebe': 23,\n",
              " 'are': 24,\n",
              " 'all': 25,\n",
              " 'as': 26,\n",
              " 'what': 27,\n",
              " 'be': 28,\n",
              " 'like': 29,\n",
              " 'no': 30,\n",
              " \"it's\": 31,\n",
              " \"i'm\": 32,\n",
              " 'her': 33,\n",
              " 'they': 34,\n",
              " 'just': 35,\n",
              " 'from': 36,\n",
              " 'okay': 37,\n",
              " 'not': 38,\n",
              " 'so': 39,\n",
              " 'my': 40,\n",
              " 'have': 41,\n",
              " 'me': 42,\n",
              " 'where': 43,\n",
              " 'know': 44,\n",
              " 'she': 45,\n",
              " 'we': 46,\n",
              " 'out': 47,\n",
              " 'well': 48,\n",
              " 'their': 49,\n",
              " 'can': 50,\n",
              " 'at': 51,\n",
              " 'he': 52,\n",
              " 'yeah': 53,\n",
              " 'your': 54,\n",
              " 'about': 55,\n",
              " 'but': 56,\n",
              " 'its': 57,\n",
              " 'up': 58,\n",
              " \"don't\": 59,\n",
              " 'text': 60,\n",
              " 'scene': 61,\n",
              " 'by': 62,\n",
              " 'do': 63,\n",
              " 'an': 64,\n",
              " 'or': 65,\n",
              " 'were': 66,\n",
              " 'there': 67,\n",
              " 'if': 68,\n",
              " 'uh': 69,\n",
              " 'look': 70,\n",
              " 'life': 71,\n",
              " 'through': 72,\n",
              " 'into': 73,\n",
              " 'him': 74,\n",
              " 'his': 75,\n",
              " \"you're\": 76,\n",
              " 'hey': 77,\n",
              " 'how': 78,\n",
              " 'right': 79,\n",
              " 'think': 80,\n",
              " 'time': 81,\n",
              " 'now': 82,\n",
              " 'paul': 83,\n",
              " 'people': 84,\n",
              " 'had': 85,\n",
              " 'world': 86,\n",
              " 'who': 87,\n",
              " \"that's\": 88,\n",
              " 'here': 89,\n",
              " 'carol': 90,\n",
              " \"y'know\": 91,\n",
              " 'see': 92,\n",
              " 'coffee': 93,\n",
              " 'ancient': 94,\n",
              " 'one': 95,\n",
              " 'got': 96,\n",
              " 'gonna': 97,\n",
              " 'some': 98,\n",
              " 'vibrant': 99,\n",
              " 'has': 100,\n",
              " 'guy': 101,\n",
              " 'get': 102,\n",
              " 'while': 103,\n",
              " 'mean': 104,\n",
              " 'could': 105,\n",
              " 'over': 106,\n",
              " 'go': 107,\n",
              " 'other': 108,\n",
              " 'land': 109,\n",
              " 'plain': 110,\n",
              " 'really': 111,\n",
              " 'way': 112,\n",
              " 'god': 113,\n",
              " 'guys': 114,\n",
              " 'air': 115,\n",
              " 'back': 116,\n",
              " 'down': 117,\n",
              " 'these': 118,\n",
              " 'used': 119,\n",
              " 'more': 120,\n",
              " 'good': 121,\n",
              " 'characters': 122,\n",
              " 'sorry': 123,\n",
              " 'barry': 124,\n",
              " 'night': 125,\n",
              " 'thing': 126,\n",
              " 'hi': 127,\n",
              " 'alan': 128,\n",
              " 'place': 129,\n",
              " 'new': 130,\n",
              " 'would': 131,\n",
              " 'also': 132,\n",
              " 'them': 133,\n",
              " 'when': 134,\n",
              " 'such': 135,\n",
              " 'phone': 136,\n",
              " 'little': 137,\n",
              " 'going': 138,\n",
              " 'only': 139,\n",
              " 'than': 140,\n",
              " 'stories': 141,\n",
              " \"there's\": 142,\n",
              " 'want': 143,\n",
              " 'um': 144,\n",
              " 'history': 145,\n",
              " 'those': 146,\n",
              " 'headline': 147,\n",
              " 'great': 148,\n",
              " 'then': 149,\n",
              " 'off': 150,\n",
              " 'everyone': 151,\n",
              " 'use': 152,\n",
              " 'change': 153,\n",
              " \"can't\": 154,\n",
              " 'sun': 155,\n",
              " 'unique': 156,\n",
              " 'will': 157,\n",
              " 'any': 158,\n",
              " 'been': 159,\n",
              " 'first': 160,\n",
              " 'feel': 161,\n",
              " \"he's\": 162,\n",
              " 'alright': 163,\n",
              " 'love': 164,\n",
              " 'why': 165,\n",
              " \"we're\": 166,\n",
              " 'cut': 167,\n",
              " 'geller': 168,\n",
              " 'last': 169,\n",
              " 'still': 170,\n",
              " 'potential': 171,\n",
              " 'concerns': 172,\n",
              " 'never': 173,\n",
              " \"didn't\": 174,\n",
              " 'starts': 175,\n",
              " 'did': 176,\n",
              " 'something': 177,\n",
              " 'word': 178,\n",
              " 'heart': 179,\n",
              " 'take': 180,\n",
              " 'book': 181,\n",
              " 'come': 182,\n",
              " 'character': 183,\n",
              " 'again': 184,\n",
              " 'room': 185,\n",
              " 'does': 186,\n",
              " 'encoding': 187,\n",
              " 'fine': 188,\n",
              " 'yes': 189,\n",
              " 'around': 190,\n",
              " 'even': 191,\n",
              " 'rich': 192,\n",
              " 'home': 193,\n",
              " 'beauty': 194,\n",
              " 'which': 195,\n",
              " 'echoes': 196,\n",
              " 'tell': 197,\n",
              " 'doing': 198,\n",
              " 'thank': 199,\n",
              " 'wait': 200,\n",
              " 'wanna': 201,\n",
              " 'big': 202,\n",
              " 'our': 203,\n",
              " 'technology': 204,\n",
              " 'need': 205,\n",
              " 'please': 206,\n",
              " 'work': 207,\n",
              " 'remember': 208,\n",
              " 'getting': 209,\n",
              " \"i'll\": 210,\n",
              " 'give': 211,\n",
              " 'enters': 212,\n",
              " 'much': 213,\n",
              " 'susan': 214,\n",
              " 'sky': 215,\n",
              " 'bustling': 216,\n",
              " 'known': 217,\n",
              " 'natural': 218,\n",
              " 'each': 219,\n",
              " 'spirit': 220,\n",
              " 'future': 221,\n",
              " 'before': 222,\n",
              " 'too': 223,\n",
              " 'reading': 224,\n",
              " \"they're\": 225,\n",
              " 'city': 226,\n",
              " 'desert': 227,\n",
              " 'traditional': 228,\n",
              " 'deep': 229,\n",
              " 'many': 230,\n",
              " 'across': 231,\n",
              " 'left': 232,\n",
              " 'data': 233,\n",
              " 'years': 234,\n",
              " 'using': 235,\n",
              " 'am': 236,\n",
              " 'us': 237,\n",
              " 'story': 238,\n",
              " \"i've\": 239,\n",
              " \"c'mon\": 240,\n",
              " 'should': 241,\n",
              " \"rachel's\": 242,\n",
              " 'filled': 243,\n",
              " 'day': 244,\n",
              " 'energy': 245,\n",
              " 'against': 246,\n",
              " 'amazon': 247,\n",
              " 'past': 248,\n",
              " 'without': 249,\n",
              " 'things': 250,\n",
              " 'ever': 251,\n",
              " 'maybe': 252,\n",
              " 'door': 253,\n",
              " 'man': 254,\n",
              " 'thanks': 255,\n",
              " 'say': 256,\n",
              " 'lizzie': 257,\n",
              " 'hidden': 258,\n",
              " 'woman': 259,\n",
              " 'high': 260,\n",
              " 'human': 261,\n",
              " 'looking': 262,\n",
              " 'social': 263,\n",
              " 'files': 264,\n",
              " 'looks': 265,\n",
              " \"isn't\": 266,\n",
              " 'file': 267,\n",
              " 'leave': 268,\n",
              " 'modern': 269,\n",
              " 'central': 270,\n",
              " 'gotta': 271,\n",
              " 'kinda': 272,\n",
              " 'la': 273,\n",
              " \"what's\": 274,\n",
              " 'live': 275,\n",
              " 'village': 276,\n",
              " 'next': 277,\n",
              " 'streets': 278,\n",
              " 'stand': 279,\n",
              " 'colorful': 280,\n",
              " 'creating': 281,\n",
              " 'every': 282,\n",
              " \"'\": 283,\n",
              " 'lost': 284,\n",
              " 'tech': 285,\n",
              " 'having': 286,\n",
              " 'make': 287,\n",
              " 'tapestry': 288,\n",
              " 'long': 289,\n",
              " 'mark': 290,\n",
              " \"i'd\": 291,\n",
              " 'takes': 292,\n",
              " 'always': 293,\n",
              " 'actually': 294,\n",
              " 'perk': 295,\n",
              " 'everybody': 296,\n",
              " 'ooh': 297,\n",
              " 'said': 298,\n",
              " \"she's\": 299,\n",
              " 'talking': 300,\n",
              " 'joanne': 301,\n",
              " 'believe': 302,\n",
              " 'music': 303,\n",
              " 'local': 304,\n",
              " 'found': 305,\n",
              " 'cultural': 306,\n",
              " 'symphony': 307,\n",
              " 'nestled': 308,\n",
              " 'sand': 309,\n",
              " 'knowledge': 310,\n",
              " 'snow': 311,\n",
              " 'peaks': 312,\n",
              " 'most': 313,\n",
              " 'amidst': 314,\n",
              " 'captivating': 315,\n",
              " 'date': 316,\n",
              " 'another': 317,\n",
              " 'ai': 318,\n",
              " 'real': 319,\n",
              " 'control': 320,\n",
              " 'let': 321,\n",
              " 'today': 322,\n",
              " 'start': 323,\n",
              " 'stop': 324,\n",
              " 'hello': 325,\n",
              " 'codes': 326,\n",
              " 'part': 327,\n",
              " 'went': 328,\n",
              " 'hand': 329,\n",
              " 'listen': 330,\n",
              " 'pheebs': 331,\n",
              " 'mrs': 332,\n",
              " 'huh': 333,\n",
              " 'kid': 334,\n",
              " 'pizza': 335,\n",
              " 'old': 336,\n",
              " 'vast': 337,\n",
              " 'glow': 338,\n",
              " 'making': 339,\n",
              " 'offering': 340,\n",
              " 'diverse': 341,\n",
              " 'seeking': 342,\n",
              " 'unfolded': 343,\n",
              " 'sahara': 344,\n",
              " 'told': 345,\n",
              " 'traditions': 346,\n",
              " 'breathtaking': 347,\n",
              " 'rugged': 348,\n",
              " 'culture': 349,\n",
              " 'growing': 350,\n",
              " 'however': 351,\n",
              " 'happy': 352,\n",
              " 'together': 353,\n",
              " 'sarah': 354,\n",
              " 'might': 355,\n",
              " 'opens': 356,\n",
              " 'anyway': 357,\n",
              " 'tv': 358,\n",
              " 'tonight': 359,\n",
              " 'window': 360,\n",
              " 'job': 361,\n",
              " 'boy': 362,\n",
              " 'parents': 363,\n",
              " 'mine': 364,\n",
              " \"let's\": 365,\n",
              " 'receptionist': 366,\n",
              " 'woven': 367,\n",
              " 'laughter': 368,\n",
              " 'hear': 369,\n",
              " 'find': 370,\n",
              " 'waiting': 371,\n",
              " 'beyond': 372,\n",
              " 'rnn': 373,\n",
              " 'model': 374,\n",
              " 'predict': 375,\n",
              " 'sounds': 376,\n",
              " 'aroma': 377,\n",
              " 'watching': 378,\n",
              " 'between': 379,\n",
              " 'generations': 380,\n",
              " 'wildlife': 381,\n",
              " 'call': 382,\n",
              " 'under': 383,\n",
              " 'cultures': 384,\n",
              " 'realm': 385,\n",
              " 'yet': 386,\n",
              " 'allowing': 387,\n",
              " 'generate': 388,\n",
              " 'descriptions': 389,\n",
              " 'villages': 390,\n",
              " 'testament': 391,\n",
              " 'nature': 392,\n",
              " 'landscapes': 393,\n",
              " 'senses': 394,\n",
              " 'dataset': 395,\n",
              " 'significant': 396,\n",
              " 'individuals': 397,\n",
              " 'whole': 398,\n",
              " 'hate': 399,\n",
              " 'development': 400,\n",
              " 'face': 401,\n",
              " 'open': 402,\n",
              " 'offer': 403,\n",
              " 'put': 404,\n",
              " 'after': 405,\n",
              " 'ring': 406,\n",
              " 'rather': 407,\n",
              " 'very': 408,\n",
              " 'david': 409,\n",
              " 'voice': 410,\n",
              " 'alone': 411,\n",
              " 'nothing': 412,\n",
              " 'drink': 413,\n",
              " 'binary': 414,\n",
              " \"'cause\": 415,\n",
              " 'two': 416,\n",
              " 'sits': 417,\n",
              " 'trying': 418,\n",
              " 'goes': 419,\n",
              " 'smoke': 420,\n",
              " 'nepal': 421,\n",
              " 'clear': 422,\n",
              " 'leaves': 423,\n",
              " 'sound': 424,\n",
              " 'stars': 425,\n",
              " 'towering': 426,\n",
              " 'centuries': 427,\n",
              " 'provided': 428,\n",
              " 'lush': 429,\n",
              " 'lives': 430,\n",
              " 'read': 431,\n",
              " 'landscape': 432,\n",
              " 'elements': 433,\n",
              " 'knew': 434,\n",
              " 'secrets': 435,\n",
              " 'silence': 436,\n",
              " 'creatures': 437,\n",
              " 'power': 438,\n",
              " 'heritage': 439,\n",
              " 'rainforest': 440,\n",
              " 'green': 441,\n",
              " 'majestic': 442,\n",
              " 'spiritual': 443,\n",
              " 'both': 444,\n",
              " 'sentences': 445,\n",
              " 'capped': 446,\n",
              " 'valleys': 447,\n",
              " 'festivals': 448,\n",
              " 'set': 449,\n",
              " 'made': 450,\n",
              " 'experience': 451,\n",
              " 'magic': 452,\n",
              " 'once': 453,\n",
              " 'journey': 454,\n",
              " 'ice': 455,\n",
              " 'may': 456,\n",
              " 'whether': 457,\n",
              " 'cancer': 458,\n",
              " 'innovation': 459,\n",
              " 'computers': 460,\n",
              " 'wrong': 461,\n",
              " 'different': 462,\n",
              " 'help': 463,\n",
              " 'thought': 464,\n",
              " 'languages': 465,\n",
              " 'readers': 466,\n",
              " 'girl': 467,\n",
              " 'end': 468,\n",
              " '1': 469,\n",
              " 'morning': 470,\n",
              " 'anything': 471,\n",
              " '8': 472,\n",
              " 'nice': 473,\n",
              " 'lapse': 474,\n",
              " 'lot': 475,\n",
              " 'dollars': 476,\n",
              " 'apartment': 477,\n",
              " 'shoe': 478,\n",
              " \"who's\": 479,\n",
              " 'talk': 480,\n",
              " 'fun': 481,\n",
              " 'five': 482,\n",
              " 'wow': 483,\n",
              " \"you've\": 484,\n",
              " 'ew': 485,\n",
              " 'robbie': 486,\n",
              " 'thumb': 487,\n",
              " 'paula': 488,\n",
              " 'forever': 489,\n",
              " 'germany': 490,\n",
              " 'trees': 491,\n",
              " 'others': 492,\n",
              " 'taking': 493,\n",
              " 'playing': 494,\n",
              " 'adorned': 495,\n",
              " 'scent': 496,\n",
              " 'books': 497,\n",
              " 'outside': 498,\n",
              " 'adventure': 499,\n",
              " 'street': 500,\n",
              " 'offered': 501,\n",
              " 'sit': 502,\n",
              " 'share': 503,\n",
              " 'family': 504,\n",
              " 'forests': 505,\n",
              " 'dunes': 506,\n",
              " 'environment': 507,\n",
              " 'created': 508,\n",
              " 'endless': 509,\n",
              " 'species': 510,\n",
              " 'remote': 511,\n",
              " 'including': 512,\n",
              " 'challenges': 513,\n",
              " 'lights': 514,\n",
              " 'ruins': 515,\n",
              " 'lived': 516,\n",
              " 'language': 517,\n",
              " 'climate': 518,\n",
              " 'diversity': 519,\n",
              " 'being': 520,\n",
              " 'warmth': 521,\n",
              " 'beaches': 522,\n",
              " 'nation': 523,\n",
              " 'researchers': 524,\n",
              " 'dr': 525,\n",
              " 'media': 526,\n",
              " 'risk': 527,\n",
              " 'remains': 528,\n",
              " 'news': 529,\n",
              " 'quantum': 530,\n",
              " 'computer': 531,\n",
              " 'information': 532,\n",
              " 'code': 533,\n",
              " 'weapons': 534,\n",
              " 'hands': 535,\n",
              " 'young': 536,\n",
              " 'working': 537,\n",
              " 'libraries': 538,\n",
              " 'spain': 539,\n",
              " 'z': 540,\n",
              " 'beans': 541,\n",
              " 'simple': 542,\n",
              " 'sometimes': 543,\n",
              " 'leaving': 544,\n",
              " \"haven't\": 545,\n",
              " 'figure': 546,\n",
              " 'markup': 547,\n",
              " 'unicode': 548,\n",
              " 'iso': 549,\n",
              " 'guess': 550,\n",
              " 'gets': 551,\n",
              " 'sex': 552,\n",
              " 'turns': 553,\n",
              " 'wish': 554,\n",
              " 'mr': 555,\n",
              " \"monica's\": 556,\n",
              " 'hair': 557,\n",
              " 'walks': 558,\n",
              " 'probably': 559,\n",
              " \"doesn't\": 560,\n",
              " 'break': 561,\n",
              " \"carol's\": 562,\n",
              " 'entering': 563,\n",
              " 'frannie': 564,\n",
              " 'thousand': 565,\n",
              " 'credits': 566,\n",
              " 'marsha': 567,\n",
              " 'already': 568,\n",
              " 'friends': 569,\n",
              " 'meet': 570,\n",
              " 'cigarette': 571,\n",
              " 'hockey': 572,\n",
              " 'kiki': 573,\n",
              " 'elara': 574,\n",
              " 'sitting': 575,\n",
              " 'park': 576,\n",
              " 'atmosphere': 577,\n",
              " 'castle': 578,\n",
              " 'within': 579,\n",
              " 'access': 580,\n",
              " 'far': 581,\n",
              " 'eye': 582,\n",
              " 'exploration': 583,\n",
              " 'windows': 584,\n",
              " 'warm': 585,\n",
              " 'corner': 586,\n",
              " 'illuminated': 587,\n",
              " 'rolling': 588,\n",
              " 'renowned': 589,\n",
              " 'glimpse': 590,\n",
              " 'enchanting': 591,\n",
              " 'cozy': 592,\n",
              " 'travelers': 593,\n",
              " 'tribes': 594,\n",
              " 'teeming': 595,\n",
              " 'markets': 596,\n",
              " 'biodiversity': 597,\n",
              " 'wonders': 598,\n",
              " 'dense': 599,\n",
              " 'living': 600,\n",
              " 'elusive': 601,\n",
              " 'mountains': 602,\n",
              " 'wind': 603,\n",
              " 'intricate': 604,\n",
              " 'often': 605,\n",
              " 'resilience': 606,\n",
              " 'celebrated': 607,\n",
              " 'beneath': 608,\n",
              " 'vivid': 609,\n",
              " 'islands': 610,\n",
              " 'machu': 611,\n",
              " 'picchu': 612,\n",
              " 'temples': 613,\n",
              " 'visitors': 614,\n",
              " 'bhutan': 615,\n",
              " 'else': 616,\n",
              " '000': 617,\n",
              " 'upon': 618,\n",
              " 'several': 619,\n",
              " 'complex': 620,\n",
              " 'discovery': 621,\n",
              " 'lead': 622,\n",
              " 'crime': 623,\n",
              " 'increased': 624,\n",
              " 'hold': 625,\n",
              " 'seen': 626,\n",
              " 'raises': 627,\n",
              " 'industry': 628,\n",
              " 'engaging': 629,\n",
              " '5g': 630,\n",
              " 'literature': 631,\n",
              " 'dream': 632,\n",
              " 'explores': 633,\n",
              " 'kill': 634,\n",
              " 'standing': 635,\n",
              " 'must': 636,\n",
              " 'hundred': 637,\n",
              " 'seven': 638,\n",
              " 'conversation': 639,\n",
              " 'shop': 640,\n",
              " 'eyes': 641,\n",
              " 'welcome': 642,\n",
              " 'try': 643,\n",
              " 'enough': 644,\n",
              " 'unfolds': 645,\n",
              " 'bring': 646,\n",
              " 'line': 647,\n",
              " 'ascii': 648,\n",
              " 'utf': 649,\n",
              " 'better': 650,\n",
              " 'someone': 651,\n",
              " 'stuff': 652,\n",
              " 'hope': 653,\n",
              " 'lesbian': 654,\n",
              " 'mom': 655,\n",
              " 'wedding': 656,\n",
              " 'sure': 657,\n",
              " 'four': 658,\n",
              " \"wouldn't\": 659,\n",
              " 'women': 660,\n",
              " 'saying': 661,\n",
              " 'stay': 662,\n",
              " \"we've\": 663,\n",
              " 'intercom': 664,\n",
              " 'wine': 665,\n",
              " 'knock': 666,\n",
              " 'bunch': 667,\n",
              " 'idea': 668,\n",
              " 'restaurant': 669,\n",
              " 'since': 670,\n",
              " 'pause': 671,\n",
              " 'exits': 672,\n",
              " 'couch': 673,\n",
              " 'scream': 674,\n",
              " 'kind': 675,\n",
              " 'yourself': 676,\n",
              " 'pregnant': 677,\n",
              " 'lasagne': 678,\n",
              " 'gave': 679,\n",
              " 'w': 680,\n",
              " 'helen': 681,\n",
              " 'smoking': 682,\n",
              " 'leslie': 683,\n",
              " 'jack': 684,\n",
              " 'puck': 685,\n",
              " 'alaria': 686,\n",
              " 'offers': 687,\n",
              " 'mingle': 688,\n",
              " 'contrasts': 689,\n",
              " 'threads': 690,\n",
              " 'cities': 691,\n",
              " 'blue': 692,\n",
              " 'beautiful': 693,\n",
              " 'along': 694,\n",
              " 'drop': 695,\n",
              " 'transformed': 696,\n",
              " 'colors': 697,\n",
              " 'perfect': 698,\n",
              " 'coming': 699,\n",
              " 'enjoy': 700,\n",
              " 'countless': 701,\n",
              " 'explore': 702,\n",
              " 'ventured': 703,\n",
              " 'further': 704,\n",
              " 'discover': 705,\n",
              " 'legends': 706,\n",
              " 'stretched': 707,\n",
              " 'dreams': 708,\n",
              " 'reality': 709,\n",
              " 'amid': 710,\n",
              " 'art': 711,\n",
              " 'international': 712,\n",
              " 'valley': 713,\n",
              " 'hills': 714,\n",
              " 'seemed': 715,\n",
              " 'cobblestone': 716,\n",
              " 'locals': 717,\n",
              " 'away': 718,\n",
              " 'sunlight': 719,\n",
              " 'solitude': 720,\n",
              " 'expanse': 721,\n",
              " 'sea': 722,\n",
              " 'harsh': 723,\n",
              " 'plant': 724,\n",
              " 'nocturnal': 725,\n",
              " 'shared': 726,\n",
              " 'pink': 727,\n",
              " 'connection': 728,\n",
              " 'explorers': 729,\n",
              " 'facing': 730,\n",
              " 'glowing': 731,\n",
              " 'chorus': 732,\n",
              " 'detailed': 733,\n",
              " 'incredible': 734,\n",
              " 'sense': 735,\n",
              " 'himalayas': 736,\n",
              " 'red': 737,\n",
              " 'fields': 738,\n",
              " 'dances': 739,\n",
              " 'resources': 740,\n",
              " 'evocative': 741,\n",
              " 'faroe': 742,\n",
              " 'determination': 743,\n",
              " 'among': 744,\n",
              " 'architecture': 745,\n",
              " 'light': 746,\n",
              " 'display': 747,\n",
              " 'humans': 748,\n",
              " 'took': 749,\n",
              " \"here's\": 750,\n",
              " 'madagascar': 751,\n",
              " 'blend': 752,\n",
              " 'spices': 753,\n",
              " 'sample': 754,\n",
              " '10': 755,\n",
              " 'full': 756,\n",
              " 'ghost': 757,\n",
              " 'developed': 758,\n",
              " 'summit': 759,\n",
              " 'rise': 760,\n",
              " 'threats': 761,\n",
              " 'capable': 762,\n",
              " 'develop': 763,\n",
              " 'algorithm': 764,\n",
              " 'communities': 765,\n",
              " 'based': 766,\n",
              " 'powerful': 767,\n",
              " 'processed': 768,\n",
              " 'food': 769,\n",
              " 'american': 770,\n",
              " 'strong': 771,\n",
              " 'least': 772,\n",
              " 'experts': 773,\n",
              " 'measures': 774,\n",
              " 'address': 775,\n",
              " 'content': 776,\n",
              " 'step': 777,\n",
              " 'ethical': 778,\n",
              " 'privacy': 779,\n",
              " 'chance': 780,\n",
              " 'number': 781,\n",
              " 'constantly': 782,\n",
              " 'programming': 783,\n",
              " 'memory': 784,\n",
              " 'programs': 785,\n",
              " 'second': 786,\n",
              " 'finally': 787,\n",
              " 'easy': 788,\n",
              " 'continues': 789,\n",
              " 'arms': 790,\n",
              " 'example': 791,\n",
              " 'done': 792,\n",
              " 'ready': 793,\n",
              " 'continue': 794,\n",
              " 'everyday': 795,\n",
              " 'important': 796,\n",
              " 'non': 797,\n",
              " 'forgotten': 798,\n",
              " 'become': 799,\n",
              " 'format': 800,\n",
              " 'importance': 801,\n",
              " 'free': 802,\n",
              " 'alchemist': 803,\n",
              " 'themes': 804,\n",
              " 'prejudice': 805,\n",
              " 'considered': 806,\n",
              " 'rings': 807,\n",
              " 'men': 808,\n",
              " 'palaces': 809,\n",
              " 'setting': 810,\n",
              " 'cup': 811,\n",
              " 'instead': 812,\n",
              " 'everything': 813,\n",
              " 'rhythmic': 814,\n",
              " 'eleanor': 815,\n",
              " 'fiery': 816,\n",
              " 'says': 817,\n",
              " 'additional': 818,\n",
              " 'keep': 819,\n",
              " 'html': 820,\n",
              " 'bit': 821,\n",
              " 'because': 822,\n",
              " 'values': 823,\n",
              " 'type': 824,\n",
              " 'turn': 825,\n",
              " 'mother': 826,\n",
              " 'myself': 827,\n",
              " 'pretty': 828,\n",
              " 'married': 829,\n",
              " \"ross's\": 830,\n",
              " 'holding': 831,\n",
              " 'wearing': 832,\n",
              " 'push': 833,\n",
              " 'stairs': 834,\n",
              " 'money': 835,\n",
              " 'sings': 836,\n",
              " 'rach': 837,\n",
              " 'ah': 838,\n",
              " 'shut': 839,\n",
              " 'commercial': 840,\n",
              " 'gives': 841,\n",
              " 'ask': 842,\n",
              " 'watch': 843,\n",
              " 'loved': 844,\n",
              " 'amazing': 845,\n",
              " 'table': 846,\n",
              " \"we'll\": 847,\n",
              " 'dead': 848,\n",
              " 'sleep': 849,\n",
              " 'card': 850,\n",
              " 'ya': 851,\n",
              " 'means': 852,\n",
              " 'butt': 853,\n",
              " 'excuse': 854,\n",
              " 'though': 855,\n",
              " \"how's\": 856,\n",
              " 'pillow': 857,\n",
              " 'tomorrow': 858,\n",
              " 'baby': 859,\n",
              " 'mindy': 860,\n",
              " 'finger': 861,\n",
              " 'whaddya': 862,\n",
              " 'fair': 863,\n",
              " 'promise': 864,\n",
              " 'buddy': 865,\n",
              " 'george': 866,\n",
              " 'omnipotent': 867,\n",
              " 'emergency': 868,\n",
              " 'brighter': 869,\n",
              " 'contradictions': 870,\n",
              " 'shines': 871,\n",
              " 'africa': 872,\n",
              " 'started': 873,\n",
              " 'canvas': 874,\n",
              " 'gathered': 875,\n",
              " 'smell': 876,\n",
              " 'gather': 877,\n",
              " 'continued': 878,\n",
              " 'walls': 879,\n",
              " 'inside': 880,\n",
              " 'halls': 881,\n",
              " 'forest': 882,\n",
              " 'base': 883,\n",
              " 'friendly': 884,\n",
              " 'alive': 885,\n",
              " 'daily': 886,\n",
              " 'skyscrapers': 887,\n",
              " 'freshly': 888,\n",
              " 'patrons': 889,\n",
              " 'urban': 890,\n",
              " 'serene': 891,\n",
              " 'lined': 892,\n",
              " 'treasures': 893,\n",
              " 'lively': 894,\n",
              " 'charming': 895,\n",
              " 'square': 896,\n",
              " 'community': 897,\n",
              " 'passed': 898,\n",
              " 'birds': 899,\n",
              " 'trails': 900,\n",
              " 'providing': 901,\n",
              " 'play': 902,\n",
              " 'heat': 903,\n",
              " 'roamed': 904,\n",
              " 'south': 905,\n",
              " 'handcrafted': 906,\n",
              " 'floor': 907,\n",
              " 'discovered': 908,\n",
              " 'frogs': 909,\n",
              " 'jungle': 910,\n",
              " 'harmony': 911,\n",
              " 'insects': 912,\n",
              " 'dark': 913,\n",
              " 'wonder': 914,\n",
              " 'monastery': 915,\n",
              " 'monks': 916,\n",
              " 'prayer': 917,\n",
              " 'mountain': 918,\n",
              " 'range': 919,\n",
              " 'color': 920,\n",
              " 'adventurers': 921,\n",
              " \"world's\": 922,\n",
              " 'behind': 923,\n",
              " 'skies': 924,\n",
              " 'canopy': 925,\n",
              " 'thrived': 926,\n",
              " 'forms': 927,\n",
              " 'waters': 928,\n",
              " 'understanding': 929,\n",
              " 'treacherous': 930,\n",
              " 'wilderness': 931,\n",
              " 'cliffs': 932,\n",
              " 'mist': 933,\n",
              " 'relentless': 934,\n",
              " 'waves': 935,\n",
              " 'weathered': 936,\n",
              " 'catch': 937,\n",
              " 'danced': 938,\n",
              " 'came': 939,\n",
              " 'mystical': 940,\n",
              " 'inca': 941,\n",
              " 'reach': 942,\n",
              " 'soft': 943,\n",
              " 'cave': 944,\n",
              " 'horizon': 945,\n",
              " 'whisper': 946,\n",
              " 'guided': 947,\n",
              " 'tradition': 948,\n",
              " 'verdant': 949,\n",
              " 'balance': 950,\n",
              " 'national': 951,\n",
              " 'pursuit': 952,\n",
              " 'monasteries': 953,\n",
              " 'chants': 954,\n",
              " 'environmental': 955,\n",
              " 'levels': 956,\n",
              " 'earth': 957,\n",
              " 'completely': 958,\n",
              " 'mouth': 959,\n",
              " 'system': 960,\n",
              " 'suggesting': 961,\n",
              " 'truly': 962,\n",
              " 'revolutionize': 963,\n",
              " 'reduce': 964,\n",
              " 'major': 965,\n",
              " 'specific': 966,\n",
              " 'developing': 967,\n",
              " 'countries': 968,\n",
              " 'powered': 969,\n",
              " 'argue': 970,\n",
              " 'developers': 971,\n",
              " 'study': 972,\n",
              " 'consumption': 973,\n",
              " 'platforms': 974,\n",
              " 'misinformation': 975,\n",
              " 'speech': 976,\n",
              " 'giants': 977,\n",
              " 'comes': 978,\n",
              " 'represent': 979,\n",
              " 'complete': 980,\n",
              " 'computing': 981,\n",
              " 'breakthrough': 982,\n",
              " 'google': 983,\n",
              " 'involved': 984,\n",
              " 'currently': 985,\n",
              " 'exhibit': 986,\n",
              " 'able': 987,\n",
              " 'allows': 988,\n",
              " 'applications': 989,\n",
              " 'companies': 990,\n",
              " 'self': 991,\n",
              " 'cybersecurity': 992,\n",
              " 'businesses': 993,\n",
              " 'protect': 994,\n",
              " 'systems': 995,\n",
              " 'steps': 996,\n",
              " 'source': 997,\n",
              " 'popularity': 998,\n",
              " \"rust's\": 999,\n",
              " 'c': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JJZBBUTFMRUu"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "\n",
        "for sentence in text_data.split('\\n'):\n",
        "  if sentence.strip() != '':\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "    for i in range(1, len(tokenized_sentence)):\n",
        "      input_sequences.append(tokenized_sentence[:i+1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-5UWFDdcPyQm"
      },
      "outputs": [],
      "source": [
        "max_len = max([len(x) for x in input_sequences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jAJgCQ-TP80J"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZN177BSPfO"
      },
      "source": [
        "## Splitting sentences into input and output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Qh-RiIxIQ16j"
      },
      "outputs": [],
      "source": [
        "X = padded_input_sequences[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PQDbf-CsRIen"
      },
      "outputs": [],
      "source": [
        "y = padded_input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jmbSNRiRRS_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ff9dc4-84be-473a-e0e2-2ef01d849933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26383, 324)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "W7y8CVMFVRNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6efcb8a-f07b-4094-9778-2b7275799894"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26383,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FNee0T7JVTPv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index)+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BW8mQkvaVusu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9860eeb-4b3a-40b1-aa14-b048428f968d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26383, 4994)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AiDmmeiuV4RG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "acJJOiMjcYX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288373ed-aa11-43f3-b507-170a0de0be14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:100: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(4994, 100, input_shape = (max_len-1,)))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(4994, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HMr2_JM4cpPw"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jGiO9OVhd6eA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "74faf786-40a9-4a3a-f97c-caa22799dd60"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m324\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m499,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)            │       \u001b[38;5;34m150,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4994\u001b[0m)           │       \u001b[38;5;34m754,094\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">324</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">499,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4994</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">754,094</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,404,094\u001b[0m (5.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,404,094</span> (5.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,404,094\u001b[0m (5.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,404,094</span> (5.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XICpYG9-joKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9d1c62-aba2-487d-87a7-ce082d59550e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 16ms/step - accuracy: 0.0495 - loss: 7.2800\n",
            "Epoch 2/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.0747 - loss: 6.4302\n",
            "Epoch 3/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.0940 - loss: 5.9325\n",
            "Epoch 4/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.1151 - loss: 5.5535\n",
            "Epoch 5/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.1402 - loss: 5.1501\n",
            "Epoch 6/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.1574 - loss: 4.7688\n",
            "Epoch 7/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.1859 - loss: 4.3933\n",
            "Epoch 8/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.2209 - loss: 4.0581\n",
            "Epoch 9/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.2589 - loss: 3.7172\n",
            "Epoch 10/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.3137 - loss: 3.4130\n",
            "Epoch 11/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.3697 - loss: 3.0989\n",
            "Epoch 12/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.4273 - loss: 2.8068\n",
            "Epoch 13/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.4819 - loss: 2.5688\n",
            "Epoch 14/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.5230 - loss: 2.3513\n",
            "Epoch 15/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.5645 - loss: 2.1382\n",
            "Epoch 16/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6011 - loss: 1.9587\n",
            "Epoch 17/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6318 - loss: 1.7932\n",
            "Epoch 18/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6720 - loss: 1.6228\n",
            "Epoch 19/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.6973 - loss: 1.4901\n",
            "Epoch 20/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.7248 - loss: 1.3703\n",
            "Epoch 21/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.7441 - loss: 1.2769\n",
            "Epoch 22/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.7639 - loss: 1.1599\n",
            "Epoch 23/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.7899 - loss: 1.0628\n",
            "Epoch 24/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8001 - loss: 1.0019\n",
            "Epoch 25/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8179 - loss: 0.9191\n",
            "Epoch 26/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8343 - loss: 0.8406\n",
            "Epoch 27/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.8400 - loss: 0.8015\n",
            "Epoch 28/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8479 - loss: 0.7430\n",
            "Epoch 29/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.8619 - loss: 0.6923\n",
            "Epoch 30/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8664 - loss: 0.6534\n",
            "Epoch 31/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.8739 - loss: 0.6104\n",
            "Epoch 32/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8811 - loss: 0.5790\n",
            "Epoch 33/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8884 - loss: 0.5500\n",
            "Epoch 34/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8949 - loss: 0.5123\n",
            "Epoch 35/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.8988 - loss: 0.4877\n",
            "Epoch 36/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9006 - loss: 0.4711\n",
            "Epoch 37/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9066 - loss: 0.4480\n",
            "Epoch 38/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9082 - loss: 0.4338\n",
            "Epoch 39/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9138 - loss: 0.4061\n",
            "Epoch 40/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9157 - loss: 0.3886\n",
            "Epoch 41/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9136 - loss: 0.3978\n",
            "Epoch 42/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9162 - loss: 0.3790\n",
            "Epoch 43/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9203 - loss: 0.3581\n",
            "Epoch 44/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.9231 - loss: 0.3466\n",
            "Epoch 45/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9226 - loss: 0.3398\n",
            "Epoch 46/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9222 - loss: 0.3354\n",
            "Epoch 47/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.9224 - loss: 0.3318\n",
            "Epoch 48/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9229 - loss: 0.3277\n",
            "Epoch 49/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9231 - loss: 0.3342\n",
            "Epoch 50/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9266 - loss: 0.3121\n",
            "Epoch 51/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9261 - loss: 0.3068\n",
            "Epoch 52/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9267 - loss: 0.2985\n",
            "Epoch 53/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9262 - loss: 0.2956\n",
            "Epoch 54/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9267 - loss: 0.2968\n",
            "Epoch 55/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2928\n",
            "Epoch 56/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9300 - loss: 0.2782\n",
            "Epoch 57/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2866\n",
            "Epoch 58/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9248 - loss: 0.2980\n",
            "Epoch 59/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9227 - loss: 0.3083\n",
            "Epoch 60/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9281 - loss: 0.2875\n",
            "Epoch 61/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9252 - loss: 0.2830\n",
            "Epoch 62/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9280 - loss: 0.2756\n",
            "Epoch 63/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9293 - loss: 0.2713\n",
            "Epoch 64/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 17ms/step - accuracy: 0.9289 - loss: 0.2746\n",
            "Epoch 65/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9278 - loss: 0.2815\n",
            "Epoch 66/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9275 - loss: 0.2768\n",
            "Epoch 67/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9248 - loss: 0.2792\n",
            "Epoch 68/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9259 - loss: 0.2728\n",
            "Epoch 69/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9289 - loss: 0.2646\n",
            "Epoch 70/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9293 - loss: 0.2666\n",
            "Epoch 71/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2760\n",
            "Epoch 72/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9305 - loss: 0.2593\n",
            "Epoch 73/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9239 - loss: 0.2757\n",
            "Epoch 74/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9264 - loss: 0.2753\n",
            "Epoch 75/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9255 - loss: 0.2797\n",
            "Epoch 76/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9252 - loss: 0.2788\n",
            "Epoch 77/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9290 - loss: 0.2578\n",
            "Epoch 78/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9292 - loss: 0.2560\n",
            "Epoch 79/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9305 - loss: 0.2496\n",
            "Epoch 80/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9267 - loss: 0.2702\n",
            "Epoch 81/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9275 - loss: 0.2605\n",
            "Epoch 82/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9263 - loss: 0.2745\n",
            "Epoch 83/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9250 - loss: 0.2758\n",
            "Epoch 84/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9304 - loss: 0.2505\n",
            "Epoch 85/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9315 - loss: 0.2492\n",
            "Epoch 86/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9296 - loss: 0.2486\n",
            "Epoch 87/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9262 - loss: 0.2692\n",
            "Epoch 88/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9310 - loss: 0.2476\n",
            "Epoch 89/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2582\n",
            "Epoch 90/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9281 - loss: 0.2648\n",
            "Epoch 91/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9235 - loss: 0.2828\n",
            "Epoch 92/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 16ms/step - accuracy: 0.9299 - loss: 0.2532\n",
            "Epoch 93/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9295 - loss: 0.2526\n",
            "Epoch 94/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9269 - loss: 0.2664\n",
            "Epoch 95/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2619\n",
            "Epoch 96/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9279 - loss: 0.2530\n",
            "Epoch 97/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9289 - loss: 0.2550\n",
            "Epoch 98/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9266 - loss: 0.2621\n",
            "Epoch 99/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9241 - loss: 0.2795\n",
            "Epoch 100/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9271 - loss: 0.2558\n",
            "Epoch 101/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9301 - loss: 0.2483\n",
            "Epoch 102/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9290 - loss: 0.2529\n",
            "Epoch 103/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9298 - loss: 0.2513\n",
            "Epoch 104/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9295 - loss: 0.2495\n",
            "Epoch 105/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9274 - loss: 0.2558\n",
            "Epoch 106/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9293 - loss: 0.2555\n",
            "Epoch 107/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9211 - loss: 0.2842\n",
            "Epoch 108/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9232 - loss: 0.2826\n",
            "Epoch 109/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9271 - loss: 0.2572\n",
            "Epoch 110/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9314 - loss: 0.2375\n",
            "Epoch 111/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9312 - loss: 0.2411\n",
            "Epoch 112/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9275 - loss: 0.2496\n",
            "Epoch 113/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9286 - loss: 0.2472\n",
            "Epoch 114/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2578\n",
            "Epoch 115/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9330 - loss: 0.2394\n",
            "Epoch 116/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9238 - loss: 0.2730\n",
            "Epoch 117/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - accuracy: 0.9238 - loss: 0.2744\n",
            "Epoch 118/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9255 - loss: 0.2618\n",
            "Epoch 119/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9242 - loss: 0.2661\n",
            "Epoch 120/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9298 - loss: 0.2455\n",
            "Epoch 121/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9314 - loss: 0.2374\n",
            "Epoch 122/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9268 - loss: 0.2534\n",
            "Epoch 123/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9290 - loss: 0.2523\n",
            "Epoch 124/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9306 - loss: 0.2476\n",
            "Epoch 125/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9313 - loss: 0.2428\n",
            "Epoch 126/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9283 - loss: 0.2573\n",
            "Epoch 127/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9150 - loss: 0.3030\n",
            "Epoch 128/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9275 - loss: 0.2567\n",
            "Epoch 129/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9293 - loss: 0.2508\n",
            "Epoch 130/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9278 - loss: 0.2562\n",
            "Epoch 131/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9291 - loss: 0.2489\n",
            "Epoch 132/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9301 - loss: 0.2478\n",
            "Epoch 133/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9301 - loss: 0.2436\n",
            "Epoch 134/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9300 - loss: 0.2412\n",
            "Epoch 135/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9234 - loss: 0.2745\n",
            "Epoch 136/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9217 - loss: 0.2831\n",
            "Epoch 137/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9267 - loss: 0.2610\n",
            "Epoch 138/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9315 - loss: 0.2446\n",
            "Epoch 139/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9270 - loss: 0.2574\n",
            "Epoch 140/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9257 - loss: 0.2589\n",
            "Epoch 141/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9269 - loss: 0.2537\n",
            "Epoch 142/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9299 - loss: 0.2445\n",
            "Epoch 143/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9303 - loss: 0.2442\n",
            "Epoch 144/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9292 - loss: 0.2483\n",
            "Epoch 145/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9294 - loss: 0.2454\n",
            "Epoch 146/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9218 - loss: 0.2809\n",
            "Epoch 147/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9253 - loss: 0.2635\n",
            "Epoch 148/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 16ms/step - accuracy: 0.9299 - loss: 0.2455\n",
            "Epoch 149/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.9267 - loss: 0.2510\n",
            "Epoch 150/150\n",
            "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9284 - loss: 0.2511\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e2a958d49e0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model.fit(X, y, epochs=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jwgpMcNQjuu0"
      },
      "outputs": [],
      "source": [
        "text = \"Please\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XcXfpvm9lA7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a95980a-f294-454e-8d7c-e9ba0d523d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step\n",
            "Please let\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me know\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Please let me know if\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me know if you\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Please let me know if you have\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me know if you have any\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me know if you have any other\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Please let me know if you have any other requests\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me know if you have any other requests or\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Please let me know if you have any other requests or would\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Please let me know if you have any other requests or would like\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Please let me know if you have any other requests or would like me\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Please let me know if you have any other requests or would like me to\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Please let me know if you have any other requests or would like me to generate\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in range (15):\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  padded_token_text = pad_sequences([token_text], maxlen = max_len, padding='pre')\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP6eWIrOBjIhWyyOQn72eMx"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}